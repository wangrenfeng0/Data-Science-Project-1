---
title: "Beers and Brewery"
author: "Renfeng Wang"
date: "10/4/2020"
output: html_document
---
```{r}
library(class)
library(caret)
library(e1071)
library(magrittr)
library(XML) 
library(dplyr)
library(tidyr)
library(stringi)
library(rvest) 
library(ggplot2)
library(RCurl) 
library(plotly)
library(gridExtra)
library(httr)
library(jsonlite)
library(e1071)
library(naniar)
library(scales)
library(gghighlight)
library(stringr)
library(knitr)
knitr::opts_chunk$set(error = TRUE)
beers=read.csv('/Users/renfengwang/Documents/SMU\ Data\ Science\ Program/Doing\ Data\ Science/Project\ 1/Beers.csv',header=T)
brewery=read.csv('/Users/renfengwang/Documents/SMU\ Data\ Science\ Program/Doing\ Data\ Science/Project\ 1/Breweries.csv',header=T)
head(beers)
head(brewery)

```
```{r}
#Question 1
brewery %>% arrange(State) %>% ggplot(aes(x=State),count=Name)+geom_bar()+geom_text(aes(label=..count..),stat='count',vjust=-.5) +
  xlab('States')+ ylab('Brewery Numbers') + ggtitle('Numbers of Brewery by State')  #Bar plot to count the brewery numbers in each state
```
```{r}
#Question 2
colnames(beers)[colnames(beers)=='Brewery_id']='Brew_ID'  #Change one of the data frame column names before merging two data sets.
df_beer=merge(beers, brewery, by='Brew_ID', all=T)  #Outer join two data frames.
colnames(df_beer)[colnames(df_beer)=='Name.x']='Beer_Name'
colnames(df_beer)[colnames(df_beer)=='Name.y']='Brewery_Name'

head(df_beer) 
tail(df_beer)
```
```{r}
#Question 3
df_beerall=df_beer %>% replace_with_na_all(condition = ~.x=='')
df_beerall=df_beerall %>% replace_with_na_all(condition = ~.x==' ')
gg_miss_var(df_beerall)

table(is.na(df_beerall$IBU))
table(is.na(df_beerall$ABV))


```
We can see there are missing values in both ABV and IBU columns.
In ABV column, there are 62 missing values.
In IBU column, there are 1005 missing values.
```{r}

#Question 4
Median_ABV_IBU=df_beerall %>% arrange(State) %>% group_by(State) %>% summarize(Median_ABV=median(ABV, na.rm=TRUE), Median_IBU=median(IBU,na.rm=TRUE))

Median_ABV_IBU %>% ggplot(aes(x=State, y=Median_ABV,width=.5))+geom_bar(stat='identity') + 
  geom_text(aes(label=percent(Median_ABV, accuracy = 0.01)),vjust=-.5,size=2.5,check_overlap = T) +
  xlab('States') +ylab('Median Alcoholic Content')+ggtitle('Median Alcoholic Content by State')

Median_ABV_IBU %>% ggplot(aes(x=State, y=Median_IBU,width=.5))+geom_bar(stat='identity') + 
  geom_text(aes(label=Median_IBU),vjust=-.5,size=2.5,check_overlap = T) +
  xlab('States') +ylab('Median International Bitterness')+ggtitle('Median International Bitterness by State') #Removed state=SD as NA

```
State South Dakota doesn't have IBU values, but has ABV values.
```{r}
#Question 5
Max_ABV_IBU=df_beerall %>% arrange(State) %>% group_by(State) %>% summarize(Max_ABV=max(ABV, na.rm=TRUE), Max_IBU=max(IBU,na.rm=T))
Max_ABV_IBU %>% ggplot(aes(x=State, y=Max_ABV,width=.5))+geom_bar(stat='identity') + 
  geom_text(aes(label=percent(Max_ABV, accuracy = 0.01)),vjust=-.5,size=2.5,check_overlap = T) +
  xlab('States') +ylab('Max Alcoholic Content')+ggtitle('Max Alcoholic Content by State')

Max_ABV_IBU %>% ggplot(aes(x=State, y=Max_IBU,width=.5))+geom_bar(stat='identity') + 
  geom_text(aes(label=Max_IBU),vjust=-.5,size=2.5,check_overlap = T) +
  xlab('States') +ylab('Max International Bitterness')+ggtitle('Max International Bitterness by State')

```
We can see Colorado has the maximum Alcoholic beer with ABV 12.8% and Oregon has the most bitter beer with IBU 138.
```{r}

#Question 6
df_beerall%>% summarize(Mean=mean(ABV, na.rm=TRUE),
                                             Median=median(ABV,na.rm=T),
                                             Min=min(ABV,na.rm=T),
                                             Max=max(ABV,na.rm=T),
                                             SD=sd(ABV,na.rm=T),
                                             N=n())

df_beer %>% filter(!is.na(ABV)) %>% ggplot(aes(x=ABV))+geom_histogram(aes(y=..density..),colour='black',fill='white')+
  geom_density(alpha=.5, fill='#FF6666')
```
The distribution of ABV is right skewed. ABV of beers around 5% has the most counts. 
There are total 2410 non-missing ABV values in this data set. The maximum ABV is 12.8%, the minimum ABV is .1%, the median ABV is 5.6%.
The mean ABV is 5.98% and standard deviation of ABV is 1.35%.
```{r}
#Question 7
df_beer %>% filter(!is.na(ABV) &!is.na(IBU)) %>% 
  ggplot(aes(y=ABV, x=IBU))+geom_point(position='jitter')+geom_smooth(method=loess)

```
Most beers with lower IBU (less than 50) have ABV values around 5%. When IBU value increases, ABV values spreads out. But most beers with IBU values above 50, their ABV values spread out within the region between 5% and 10%.
```{r}
#Question 8  KNN
df_beer_IPA=df_beer %>% filter(!is.na(ABV) &!is.na(IBU)) %>% 
  filter(str_detect(Style, regex(str_c('\\b','IPA','\\b',sep=''), ignore_case = T)))

df_beer_IPA$Style=as.factor('IPA')

df_beer_Ale=df_beer %>% filter(!is.na(ABV) &!is.na(IBU)) %>% 
  filter(str_detect(Style, regex(str_c('\\b','Ale','\\b',sep=''), ignore_case = T)))

df_beer_Ale$Style=as.factor('Ale')

df_beer_test=rbind(df_beer_IPA, df_beer_Ale)

iterations = 500
numks = 30
splitPerc = .7
masterAcc = matrix(nrow = iterations, ncol = numks)
set.seed(6)
trainIndices = sample(1:dim(df_beer_test)[1],round(splitPerc * dim(df_beer_test)[1]))
beer_train = df_beer_test[trainIndices,]
beer_test = df_beer_test[-trainIndices,]
classifications=knn(beer_train[,c(4,5)],beer_test[,c(4,5)],beer_train$Style, prob = TRUE, k = 5)
CM = confusionMatrix(table(classifications,beer_test$Style))
classifications
CM
```
I randomly assigned 70% of original data to training data and 30% of the original data as testing data when seed value is set to 6.
We can tell when k=5, the accuracy is about 82.81%.
```{r}
for(j in 1:iterations)
{
  accs = data.frame(accuracy = numeric(30), k = numeric(30))
  trainIndices = sample(1:dim(df_beer_test)[1],round(splitPerc * dim(df_beer_test)[1]))
  beer_train = df_beer_test[trainIndices,]
  beer_test = df_beer_test[-trainIndices,]
  for(i in 1:numks)
  {
    classifications = knn(beer_train[,c(4,5)],beer_test[,c(4,5)],beer_train$Style, prob = TRUE, k = i)
    table(classifications,beer_test$Style)
    CM = confusionMatrix(table(classifications,beer_test$Style))
    masterAcc[j,i] = CM$overall[1]
  }
}
MeanAcc = colMeans(masterAcc)
p=ggplot(mapping=aes(x=seq(1,numks,1), y=MeanAcc))+geom_line()
ggplotly(p)

```
I shuffled the training and testing data 500 times by 70%/30% split. And assigned integer k value from 1 to 30. 
From the plot, we can tell when k=5, it gives us the highest accuracy 85% which means we can predict the beer is either India Pale Ales or
any other types of Ale by knowing its ABV and IBU values with 85% accuracy when sets nearest neighbor numbers equals to 5
```{r}
#Naive Bayes--------------

iterations = 500
masterAcc = matrix(nrow = iterations)
masterSen = matrix(nrow = iterations)
masterSpec = matrix(nrow = iterations)
splitPerc = .7 
set.seed(6)
for(j in 1:iterations)
{
  
  trainIndices = sample(1:dim(df_beer_test)[1],round(splitPerc * dim(df_beer_test)[1]))
  beer_train = df_beer_test[trainIndices,]
  beer_test = df_beer_test[-trainIndices,]
  model = naiveBayes(beer_train[,c(4,5)],as.factor(beer_train$Style),laplace = 1)
  table(predict(model,beer_test[,c(4,5)]),as.factor(beer_test$Style))
  CM = confusionMatrix(table(predict(model,beer_test[,c(4,5)]),as.factor(beer_test$Style)))
  masterAcc[j] = CM$overall[1]
  masterSen[j] = CM$byClass[1]
  masterSpec[j] = CM$byClass[2]
}
MeanAcc = colMeans(masterAcc)
MeanSen = colMeans(masterSen)
MeanSpec = colMeans(masterSpec)
MeanAcc
MeanSen
MeanSpec

```
Now I chose to use Naive Bayes model as I want to compare the accuracy with KNN model. I shuffled the training and testing data 500 times by 70%/30% split like I did in KNN model. The mean accuracy of Naive Bayes is 84% which is similar to what we got from KNN.
```{r}
#Question 9 Sorted Lager, Stout, IPA
df_beer_stout=df_beer %>% filter(!is.na(ABV) &!is.na(IBU)) %>% 
  filter(str_detect(Style, regex(str_c('\\b','stout','\\b',sep=''), ignore_case = T)))

df_beer_lager=df_beer %>% filter(!is.na(ABV) &!is.na(IBU)) %>% 
  filter(str_detect(Style, regex(str_c('\\b','lager','\\b',sep=''), ignore_case = T)))

df_beer_stout$Style=as.factor('Stout')
df_beer_lager$Style=as.factor('Lager')

df_beer_sort=rbind(df_beer_IPA, df_beer_stout)
df_beer_sort=rbind(df_beer_sort, df_beer_lager)

iterations = 500
masterAcc = matrix(nrow = iterations)
masterSen = matrix(nrow = iterations)
masterSpec = matrix(nrow = iterations)
splitPerc = .7 
set.seed(6)
for(j in 1:iterations)
{
  
  trainIndices = sample(1:dim(df_beer_sort)[1],round(splitPerc * dim(df_beer_sort)[1]))
  beer_sort_train = df_beer_sort[trainIndices,]
  beer_sort_test = df_beer_sort[-trainIndices,]
  model = naiveBayes(beer_sort_train[,c(4,5)],as.factor(beer_sort_train$Style),laplace = 1)
  table(predict(model,beer_sort_test[,c(4,5)]),as.factor(beer_sort_test$Style))
  CM = confusionMatrix(table(predict(model,beer_sort_test[,c(4,5)]),as.factor(beer_sort_test$Style)))
  masterAcc[j] = CM$overall[1]
  masterSen[j] = CM$byClass[1]
  masterSpec[j] = CM$byClass[2]
}
MeanAcc = colMeans(masterAcc)
MeanSen = colMeans(masterSen)
MeanSpec = colMeans(masterSpec)
MeanAcc
MeanSen
MeanSpec

```
Now I want to use IBU and ABV values to predict non-IPA Ales, lager and stout these three beer styles. I shuffled the training and testing data 500 times by 70%/30% split like I did before. First I used Naive Bayes method. The mean accuracy is about 84.5%.
```{r}

iterations = 500
numks = 30
splitPerc = .7
masterAcc = matrix(nrow = iterations, ncol = numks)
set.seed(6)
for(j in 1:iterations)
{
  accs = data.frame(accuracy = numeric(30), k = numeric(30))
  trainIndices = sample(1:dim(df_beer_sort)[1],round(splitPerc * dim(df_beer_sort)[1]))
  beer_sort_train = df_beer_sort[trainIndices,]
  beer_sort_test = df_beer_sort[-trainIndices,]
  for(i in 1:numks)
  {
    classifications = knn(beer_sort_train[,c(4,5)],beer_sort_test[,c(4,5)],beer_sort_train$Style, prob = TRUE, k = i)
    table(classifications,beer_sort_test$Style)
    CM = confusionMatrix(table(classifications,beer_sort_test$Style))
    masterAcc[j,i] = CM$overall[1]
  }
}
MeanAcc = colMeans(masterAcc)
e=ggplot(mapping=aes(x=seq(1,numks,1), y=MeanAcc))+geom_line()
ggplotly(e)
MeanAcc
```
Now I used KNN method. The highest mean accuracy is 87.9% when k=1.We can see from the plot that accuracy dropped when k=2, but increased when 
k>=6. Then the accuracy keeps arising. 
```{r}
set.seed(4)
trainIndices = sample(1:dim(df_beer_sort)[1],round(splitPerc * dim(df_beer_sort)[1]))
beer_sort_train = df_beer_sort[trainIndices,]
beer_sort_test = df_beer_sort[-trainIndices,]
classifications_knn=knn(beer_sort_train[,c(4,5)],beer_sort_test[,c(4,5)],beer_sort_train$Style, prob = TRUE, k = 5)
CM_knn = confusionMatrix(table(classifications,beer_sort_test$Style))
classifications_knn
CM_knn
```
Here is one of the case I shuffled the testing and training data once and the accuracy we got is 85% when k=5
```{r}
#Back to Question 7
df_beer_study=rbind(df_beer_sort, df_beer_Ale)
df_beer_study %>% ggplot(aes(y=ABV, x=IBU))+geom_point(position='jitter')+geom_smooth() +facet_wrap(~Style)
```
  Now we came back to Question 7 by checking four different styles of beer. 
  We can see, majority of lager style beer has low IBU and its ABV values are around 5%. No ABV values of lager beer are above 7.5%.
  ABV values of Indian Pale Ale style seems like increases when IBU increases, but their ABV values don't pass 10% and majority of their ABV values are between 5% to 10%.
  The other types of Ale beer have low IBU values which most of them are below 50. Also, most of their ABV values are between 3.75% to 10%.
  The data size of stout style beer is small, but we can roughly tell their ABV increases when IBU increases.
